{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Demo\n",
    "\n",
    "Based on this demo: http://nlpforhackers.io/language-models/\n",
    "\n",
    "### Import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import reuters, movie_reviews, shakespeare, brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedTaggedCorpusReader in u'.../corpora/brown' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "# Choose a corpus: reuters, movie_reviews or shakespeare\n",
    "corpus = brown\n",
    "print  corpus\n",
    "if corpus==shakespeare:\n",
    "    shakespeare_text = ''.join([''.join(corpus.xml(fileid).itertext()) for fileid in corpus.fileids()])\n",
    "    words = word_tokenize(shakespeare_text)\n",
    "    sents = [word_tokenize(sent) for sent in sent_tokenize(shakespeare_text)]\n",
    "else:    \n",
    "    words = corpus.words()\n",
    "    sents = corpus.sents()\n",
    "\n",
    "# Lowercase everything\n",
    "words = [w.lower() for w in words]\n",
    "sents = [[w.lower() for w in sent] for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "In this section, we will construct a language model based on unigrams (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of words in corpus:  1161192\n",
      "\n",
      "Top 10 most common words: \n",
      "the 69971\n",
      ", 58334\n",
      ". 49346\n",
      "of 36412\n",
      "and 28853\n",
      "to 26158\n",
      "a 23195\n",
      "in 21337\n",
      "that 10594\n",
      "is 10109\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1. Fill in the blanks.\n",
    "\n",
    "# Step 1: Make a Counter from the list of words and call it \"unigram_counts\" (remember, this is easy to do!)\n",
    "unigram_counts = Counter(words)\n",
    "\n",
    "# Step 2: Get the total number of words and assign it to \"total_count\"\n",
    "total_count = sum(unigram_counts.values())\n",
    "\n",
    "print \"Total number of words in corpus: \", total_count\n",
    "\n",
    "# Print 10 most common words\n",
    "print \"\\nTop 10 most common words: \"\n",
    "for (word, count) in unigram_counts.most_common(n=10):\n",
    "    print word, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities sum to:  1.0\n",
      "\n",
      "Top 10 most common words: \n",
      "the 0.06026\n",
      ", 0.05024\n",
      ". 0.04250\n",
      "of 0.03136\n",
      "and 0.02485\n",
      "to 0.02253\n",
      "a 0.01998\n",
      "in 0.01838\n",
      "that 0.00912\n",
      "is 0.00871\n",
      "was 0.00845\n",
      "he 0.00822\n",
      "for 0.00817\n",
      "`` 0.00761\n",
      "'' 0.00757\n",
      "it 0.00754\n",
      "with 0.00628\n",
      "as 0.00625\n",
      "his 0.00602\n",
      "on 0.00581\n",
      "be 0.00549\n",
      "; 0.00479\n",
      "at 0.00463\n",
      "by 0.00457\n",
      "i 0.00445\n",
      "this 0.00443\n",
      "had 0.00442\n",
      "? 0.00404\n",
      "not 0.00397\n",
      "are 0.00378\n",
      "but 0.00377\n",
      "from 0.00376\n",
      "or 0.00362\n",
      "have 0.00339\n",
      "an 0.00322\n",
      "they 0.00312\n",
      "which 0.00307\n",
      "-- 0.00296\n",
      "one 0.00284\n",
      "you 0.00283\n",
      "were 0.00283\n",
      "her 0.00261\n",
      "all 0.00258\n",
      "she 0.00246\n",
      "there 0.00235\n",
      "would 0.00234\n",
      "their 0.00230\n",
      "we 0.00228\n",
      "him 0.00226\n",
      "been 0.00213\n",
      ") 0.00212\n",
      "has 0.00210\n",
      "( 0.00210\n",
      "when 0.00201\n",
      "who 0.00194\n",
      "will 0.00193\n",
      "more 0.00191\n",
      "if 0.00189\n",
      "no 0.00184\n",
      "out 0.00181\n",
      "so 0.00171\n",
      "said 0.00169\n",
      "what 0.00164\n",
      "up 0.00163\n",
      "its 0.00160\n",
      "about 0.00156\n",
      ": 0.00155\n",
      "into 0.00154\n",
      "than 0.00154\n",
      "them 0.00154\n",
      "can 0.00153\n",
      "only 0.00151\n",
      "other 0.00147\n",
      "new 0.00141\n",
      "some 0.00139\n",
      "could 0.00138\n",
      "time 0.00138\n",
      "! 0.00137\n",
      "these 0.00135\n",
      "two 0.00122\n",
      "may 0.00121\n",
      "then 0.00119\n",
      "do 0.00117\n",
      "first 0.00117\n",
      "any 0.00116\n",
      "my 0.00114\n",
      "now 0.00113\n",
      "such 0.00112\n",
      "like 0.00111\n",
      "our 0.00108\n",
      "over 0.00106\n",
      "man 0.00104\n",
      "me 0.00102\n",
      "even 0.00101\n",
      "most 0.00100\n",
      "made 0.00097\n",
      "also 0.00092\n",
      "after 0.00092\n",
      "did 0.00090\n",
      "many 0.00089\n",
      "before 0.00087\n",
      "must 0.00087\n",
      "af 0.00086\n",
      "through 0.00084\n",
      "back 0.00083\n",
      "years 0.00082\n",
      "much 0.00081\n",
      "where 0.00081\n",
      "your 0.00079\n",
      "way 0.00078\n",
      "well 0.00077\n",
      "down 0.00077\n",
      "should 0.00076\n",
      "because 0.00076\n",
      "each 0.00076\n",
      "just 0.00075\n",
      "those 0.00073\n",
      "people 0.00073\n",
      "mr. 0.00073\n",
      "too 0.00072\n",
      "how 0.00072\n",
      "little 0.00072\n",
      "state 0.00069\n",
      "good 0.00069\n",
      "very 0.00069\n",
      "make 0.00068\n",
      "world 0.00068\n",
      "still 0.00067\n",
      "see 0.00066\n",
      "own 0.00066\n",
      "men 0.00066\n",
      "work 0.00066\n",
      "long 0.00065\n",
      "here 0.00065\n",
      "get 0.00065\n",
      "between 0.00063\n",
      "both 0.00063\n",
      "life 0.00062\n",
      "being 0.00061\n",
      "under 0.00061\n",
      "never 0.00060\n",
      "day 0.00059\n",
      "same 0.00059\n",
      "another 0.00059\n",
      "know 0.00059\n",
      "while 0.00059\n",
      "last 0.00058\n",
      "us 0.00058\n",
      "might 0.00058\n",
      "great 0.00057\n",
      "old 0.00057\n",
      "year 0.00057\n",
      "off 0.00055\n",
      "come 0.00054\n",
      "since 0.00054\n",
      "against 0.00054\n",
      "go 0.00054\n",
      "came 0.00054\n",
      "right 0.00053\n",
      "used 0.00053\n",
      "take 0.00053\n",
      "three 0.00053\n",
      "himself 0.00052\n",
      "states 0.00052\n",
      "few 0.00052\n",
      "house 0.00051\n",
      "use 0.00051\n",
      "during 0.00050\n",
      "without 0.00050\n",
      "again 0.00050\n",
      "place 0.00049\n",
      "american 0.00049\n",
      "around 0.00048\n",
      "however 0.00048\n",
      "home 0.00047\n",
      "small 0.00047\n",
      "found 0.00046\n",
      "mrs. 0.00046\n",
      "1 0.00045\n",
      "thought 0.00045\n",
      "went 0.00044\n",
      "say 0.00043\n",
      "part 0.00043\n",
      "once 0.00043\n",
      "general 0.00043\n",
      "high 0.00043\n",
      "upon 0.00043\n",
      "school 0.00042\n",
      "every 0.00042\n",
      "don't 0.00042\n",
      "does 0.00042\n",
      "got 0.00042\n",
      "united 0.00042\n",
      "left 0.00041\n",
      "number 0.00041\n",
      "course 0.00040\n",
      "war 0.00040\n",
      "until 0.00040\n",
      "always 0.00039\n",
      "away 0.00039\n",
      "something 0.00039\n",
      "fact 0.00038\n",
      "2 0.00038\n",
      "water 0.00038\n",
      "though 0.00038\n",
      "public 0.00038\n",
      "put 0.00038\n",
      "less 0.00038\n",
      "think 0.00037\n",
      "almost 0.00037\n",
      "hand 0.00037\n",
      "enough 0.00037\n",
      "took 0.00037\n",
      "far 0.00037\n",
      "head 0.00037\n",
      "yet 0.00036\n",
      "government 0.00036\n",
      "system 0.00036\n",
      "set 0.00036\n",
      "better 0.00036\n",
      "told 0.00036\n",
      "night 0.00035\n",
      "nothing 0.00035\n",
      "end 0.00035\n",
      "why 0.00035\n",
      "didn't 0.00035\n",
      "eyes 0.00035\n",
      "called 0.00035\n",
      "find 0.00034\n",
      "look 0.00034\n",
      "going 0.00034\n",
      "asked 0.00034\n",
      "later 0.00034\n",
      "point 0.00034\n",
      "knew 0.00034\n",
      "next 0.00034\n",
      "program 0.00034\n",
      "city 0.00034\n",
      "business 0.00034\n",
      "group 0.00034\n",
      "give 0.00034\n",
      "toward 0.00033\n",
      "young 0.00033\n",
      "room 0.00033\n",
      "days 0.00033\n",
      "let 0.00033\n",
      "president 0.00033\n",
      "side 0.00033\n",
      "social 0.00033\n",
      "several 0.00032\n",
      "given 0.00032\n",
      "present 0.00032\n",
      "order 0.00032\n",
      "national 0.00032\n",
      "possible 0.00032\n",
      "rather 0.00032\n",
      "second 0.00032\n",
      "per 0.00032\n",
      "face 0.00032\n",
      "among 0.00032\n",
      "form 0.00032\n",
      "important 0.00032\n",
      "often 0.00032\n",
      "things 0.00032\n",
      "looked 0.00032\n",
      "early 0.00032\n",
      "white 0.00031\n",
      "john 0.00031\n",
      "case 0.00031\n",
      "large 0.00031\n",
      "four 0.00031\n",
      "need 0.00031\n",
      "big 0.00031\n",
      "become 0.00031\n",
      "within 0.00031\n",
      "felt 0.00031\n",
      "along 0.00031\n",
      "children 0.00031\n",
      "saw 0.00030\n",
      "best 0.00030\n",
      "church 0.00030\n",
      "ever 0.00030\n",
      "least 0.00030\n",
      "power 0.00029\n",
      "development 0.00029\n",
      "seemed 0.00029\n",
      "thing 0.00029\n",
      "light 0.00029\n",
      "family 0.00029\n",
      "interest 0.00028\n",
      "want 0.00028\n",
      "mind 0.00028\n",
      "members 0.00028\n",
      "country 0.00028\n",
      "area 0.00028\n",
      "others 0.00028\n",
      "although 0.00028\n",
      "turned 0.00028\n",
      "done 0.00027\n",
      "open 0.00027\n",
      "' 0.00027\n",
      "god 0.00027\n",
      "service 0.00027\n",
      "problem 0.00027\n",
      "kind 0.00027\n",
      "certain 0.00027\n",
      "door 0.00027\n",
      "began 0.00027\n",
      "thus 0.00027\n",
      "different 0.00027\n",
      "sense 0.00027\n",
      "help 0.00027\n",
      "means 0.00027\n",
      "whole 0.00027\n",
      "matter 0.00027\n",
      "perhaps 0.00026\n",
      "itself 0.00026\n",
      "it's 0.00026\n",
      "york 0.00026\n",
      "times 0.00026\n",
      "human 0.00026\n",
      "law 0.00026\n",
      "line 0.00026\n",
      "above 0.00025\n",
      "name 0.00025\n",
      "example 0.00025\n",
      "action 0.00025\n",
      "company 0.00025\n",
      "hands 0.00025\n",
      "local 0.00025\n",
      "show 0.00025\n",
      "3 0.00025\n",
      "history 0.00025\n",
      "whether 0.00025\n",
      "five 0.00025\n",
      "gave 0.00025\n",
      "today 0.00024\n",
      "either 0.00024\n",
      "act 0.00024\n",
      "feet 0.00024\n",
      "across 0.00024\n",
      "past 0.00024\n",
      "quite 0.00024\n",
      "taken 0.00024\n",
      "anything 0.00024\n",
      "having 0.00024\n",
      "seen 0.00024\n",
      "death 0.00024\n",
      "experience 0.00024\n",
      "body 0.00024\n",
      "week 0.00024\n",
      "half 0.00024\n",
      "really 0.00024\n",
      "field 0.00024\n",
      "car 0.00024\n",
      "word 0.00024\n",
      "words 0.00024\n",
      "already 0.00024\n",
      "themselves 0.00023\n",
      "i'm 0.00023\n",
      "information 0.00023\n",
      "tell 0.00023\n",
      "shall 0.00023\n",
      "together 0.00023\n",
      "college 0.00023\n",
      "period 0.00023\n",
      "money 0.00023\n",
      "keep 0.00023\n",
      "held 0.00023\n",
      "sure 0.00023\n",
      "probably 0.00022\n",
      "seems 0.00022\n",
      "free 0.00022\n",
      "real 0.00022\n",
      "behind 0.00022\n",
      "cannot 0.00022\n",
      "political 0.00022\n",
      "question 0.00022\n",
      "air 0.00022\n",
      "office 0.00022\n",
      "making 0.00022\n",
      "miss 0.00022\n",
      "brought 0.00022\n",
      "whose 0.00022\n",
      "special 0.00022\n",
      "heard 0.00021\n",
      "problems 0.00021\n",
      "major 0.00021\n",
      "study 0.00021\n",
      "moment 0.00021\n",
      "federal 0.00021\n",
      "ago 0.00021\n",
      "became 0.00021\n",
      "available 0.00021\n",
      "known 0.00021\n",
      "street 0.00021\n",
      "result 0.00021\n",
      "economic 0.00021\n",
      "boy 0.00021\n",
      "reason 0.00021\n",
      "position 0.00021\n",
      "change 0.00021\n",
      "south 0.00021\n",
      "individual 0.00021\n",
      "board 0.00021\n",
      "job 0.00020\n",
      "am 0.00020\n",
      "society 0.00020\n",
      "areas 0.00020\n",
      "west 0.00020\n",
      "close 0.00020\n",
      "turn 0.00020\n",
      "love 0.00020\n",
      "community 0.00020\n",
      "true 0.00020\n",
      "full 0.00020\n",
      "force 0.00020\n",
      "court 0.00020\n",
      "cost 0.00020\n",
      "seem 0.00020\n",
      "wife 0.00020\n",
      "future 0.00020\n",
      "age 0.00020\n",
      "voice 0.00019\n",
      "wanted 0.00019\n",
      "department 0.00019\n",
      "woman 0.00019\n",
      "center 0.00019\n",
      "common 0.00019\n",
      "control 0.00019\n",
      "necessary 0.00019\n",
      "policy 0.00019\n",
      "following 0.00019\n",
      "sometimes 0.00019\n",
      "front 0.00019\n",
      "girl 0.00019\n",
      "six 0.00019\n",
      "clear 0.00019\n",
      "land 0.00019\n",
      "further 0.00019\n",
      "music 0.00019\n",
      "party 0.00019\n",
      "able 0.00019\n",
      "provide 0.00019\n",
      "mother 0.00019\n",
      "feel 0.00019\n",
      "university 0.00018\n",
      "education 0.00018\n",
      "level 0.00018\n",
      "effect 0.00018\n",
      "child 0.00018\n",
      "students 0.00018\n",
      "short 0.00018\n",
      "military 0.00018\n",
      "run 0.00018\n",
      "stood 0.00018\n",
      "town 0.00018\n",
      "total 0.00018\n",
      "morning 0.00018\n",
      "outside 0.00018\n",
      "figure 0.00018\n",
      "rate 0.00018\n",
      "art 0.00018\n",
      "century 0.00018\n",
      "class 0.00018\n",
      "4 0.00018\n",
      "usually 0.00018\n",
      "washington 0.00018\n",
      "north 0.00018\n",
      "therefore 0.00018\n",
      "plan 0.00018\n",
      "leave 0.00018\n",
      "sound 0.00018\n",
      "million 0.00018\n",
      "evidence 0.00018\n",
      "top 0.00018\n",
      "black 0.00017\n",
      "strong 0.00017\n",
      "hard 0.00017\n",
      "tax 0.00017\n",
      "various 0.00017\n",
      "surface 0.00017\n",
      "value 0.00017\n",
      "play 0.00017\n",
      "type 0.00017\n",
      "says 0.00017\n",
      "believe 0.00017\n",
      "soon 0.00017\n",
      "mean 0.00017\n",
      "peace 0.00017\n",
      "near 0.00017\n",
      "lines 0.00017\n",
      "modern 0.00017\n",
      "table 0.00017\n",
      "red 0.00017\n",
      "road 0.00017\n",
      "book 0.00017\n",
      "situation 0.00017\n",
      "process 0.00017\n",
      "personal 0.00017\n",
      "minutes 0.00017\n",
      "nor 0.00017\n",
      "gone 0.00017\n",
      "idea 0.00017\n",
      "increase 0.00017\n",
      "alone 0.00017\n",
      "schools 0.00017\n",
      "women 0.00017\n",
      "english 0.00017\n",
      "america 0.00017\n",
      "started 0.00017\n",
      "living 0.00017\n",
      "longer 0.00017\n",
      "cut 0.00017\n",
      "dr. 0.00017\n",
      "nature 0.00016\n",
      "secretary 0.00016\n",
      "private 0.00016\n",
      "finally 0.00016\n",
      "third 0.00016\n",
      "section 0.00016\n",
      "months 0.00016\n",
      "greater 0.00016\n",
      "call 0.00016\n",
      "needed 0.00016\n",
      "fire 0.00016\n",
      "that's 0.00016\n",
      "expected 0.00016\n",
      "ground 0.00016\n",
      "values 0.00016\n",
      "kept 0.00016\n",
      "view 0.00016\n",
      "pressure 0.00016\n",
      "dark 0.00016\n",
      "everything 0.00016\n",
      "basis 0.00016\n",
      "space 0.00016\n",
      "father 0.00016\n",
      "east 0.00016\n",
      "union 0.00016\n",
      "spirit 0.00016\n",
      "required 0.00016\n",
      "complete 0.00016\n",
      "wrote 0.00016\n",
      "moved 0.00016\n",
      "except 0.00016\n",
      "i'll 0.00016\n",
      "conditions 0.00016\n",
      "support 0.00016\n",
      "return 0.00016\n",
      "recent 0.00015\n",
      "late 0.00015\n",
      "particular 0.00015\n",
      "attention 0.00015\n",
      "hope 0.00015\n",
      "live 0.00015\n",
      "else 0.00015\n",
      "costs 0.00015\n",
      "brown 0.00015\n",
      "couldn't 0.00015\n",
      "stage 0.00015\n",
      "taking 0.00015\n",
      "nations 0.00015\n",
      "forces 0.00015\n",
      "beyond 0.00015\n",
      "read 0.00015\n",
      "coming 0.00015\n",
      "hours 0.00015\n",
      "inside 0.00015\n",
      "person 0.00015\n",
      "report 0.00015\n",
      "material 0.00015\n",
      "dead 0.00015\n",
      "lost 0.00015\n",
      "miles 0.00015\n",
      "data 0.00015\n",
      "heart 0.00015\n",
      "looking 0.00015\n",
      "low 0.00015\n",
      "instead 0.00015\n",
      "feeling 0.00015\n",
      "followed 0.00015\n",
      "makes 0.00015\n",
      "1960 0.00015\n",
      "added 0.00015\n",
      "single 0.00015\n",
      "pay 0.00015\n",
      "amount 0.00015\n",
      "research 0.00015\n",
      "hundred 0.00015\n",
      "basic 0.00015\n",
      "industry 0.00015\n",
      "cold 0.00015\n",
      "move 0.00015\n",
      "simply 0.00015\n",
      "including 0.00015\n",
      "tried 0.00015\n",
      "developed 0.00015\n",
      "can't 0.00015\n",
      "reached 0.00015\n",
      "hold 0.00015\n",
      "committee 0.00014\n",
      "equipment 0.00014\n",
      "defense 0.00014\n",
      "island 0.00014\n",
      "shown 0.00014\n",
      "actually 0.00014\n",
      "religious 0.00014\n",
      "central 0.00014\n",
      "son 0.00014\n",
      "river 0.00014\n",
      "beginning 0.00014\n",
      "ten 0.00014\n",
      "sort 0.00014\n",
      "st. 0.00014\n",
      "getting 0.00014\n",
      "trying 0.00014\n",
      "& 0.00014\n",
      "rest 0.00014\n",
      "terms 0.00014\n",
      "doing 0.00014\n",
      "received 0.00014\n",
      "u.s. 0.00014\n",
      "picture 0.00014\n",
      "indeed 0.00014\n",
      "medical 0.00014\n",
      "care 0.00014\n",
      "friends 0.00014\n",
      "especially 0.00014\n",
      "difficult 0.00014\n",
      "subject 0.00014\n",
      "fine 0.00014\n",
      "administration 0.00014\n",
      "higher 0.00014\n",
      "simple 0.00014\n",
      "wall 0.00014\n",
      "building 0.00014\n",
      "walked 0.00014\n",
      "meeting 0.00014\n",
      "foreign 0.00014\n",
      "bring 0.00014\n",
      "floor 0.00014\n",
      "paper 0.00014\n",
      "similar 0.00014\n",
      "range 0.00014\n",
      "passed 0.00014\n",
      "final 0.00013\n",
      "natural 0.00013\n",
      "property 0.00013\n",
      "training 0.00013\n",
      "cent 0.00013\n",
      "international 0.00013\n",
      "police 0.00013\n",
      "county 0.00013\n",
      "growth 0.00013\n",
      "market 0.00013\n",
      "start 0.00013\n",
      "written 0.00013\n",
      "wasn't 0.00013\n",
      "talk 0.00013\n",
      "england 0.00013\n",
      "suddenly 0.00013\n",
      "story 0.00013\n",
      "hear 0.00013\n",
      "needs 0.00013\n",
      "hall 0.00013\n",
      "answer 0.00013\n",
      "issue 0.00013\n",
      "10 0.00013\n",
      "congress 0.00013\n",
      "working 0.00013\n",
      "you're 0.00013\n",
      "likely 0.00013\n",
      "countries 0.00013\n",
      "considered 0.00013\n",
      "sat 0.00013\n",
      "earth 0.00013\n",
      "entire 0.00013\n",
      "results 0.00013\n",
      "labor 0.00013\n",
      "purpose 0.00013\n",
      "meet 0.00013\n",
      "happened 0.00013\n",
      "production 0.00013\n",
      "stand 0.00013\n",
      "william 0.00013\n",
      "hair 0.00013\n",
      "cases 0.00013\n",
      "difference 0.00013\n",
      "food 0.00013\n",
      "fall 0.00013\n",
      "stock 0.00013\n",
      "involved 0.00013\n",
      "earlier 0.00013\n",
      "increased 0.00013\n",
      "particularly 0.00013\n",
      "whom 0.00013\n",
      "hour 0.00012\n",
      "knowledge 0.00012\n",
      "letter 0.00012\n",
      "sent 0.00012\n",
      "effort 0.00012\n",
      "paid 0.00012\n",
      "below 0.00012\n",
      "club 0.00012\n",
      "thinking 0.00012\n",
      "using 0.00012\n",
      "yes 0.00012\n",
      "christian 0.00012\n",
      "deal 0.00012\n",
      "blue 0.00012\n",
      "ideas 0.00012\n",
      "points 0.00012\n",
      "bill 0.00012\n",
      "square 0.00012\n",
      "certainly 0.00012\n",
      "ready 0.00012\n",
      "boys 0.00012\n",
      "trade 0.00012\n",
      "industrial 0.00012\n",
      "methods 0.00012\n",
      "bad 0.00012\n",
      "addition 0.00012\n",
      "girls 0.00012\n",
      "due 0.00012\n",
      "method 0.00012\n",
      "moral 0.00012\n",
      "5 0.00012\n",
      "neither 0.00012\n",
      "reading 0.00012\n",
      "throughout 0.00012\n",
      "showed 0.00012\n",
      "directly 0.00012\n",
      "weeks 0.00012\n",
      "decided 0.00012\n",
      "nearly 0.00012\n",
      "statement 0.00012\n",
      "questions 0.00012\n",
      "according 0.00012\n",
      "color 0.00012\n",
      "try 0.00012\n",
      "anyone 0.00012\n",
      "kennedy 0.00012\n",
      "services 0.00012\n",
      "french 0.00012\n",
      "nation 0.00012\n",
      "lay 0.00012\n",
      "programs 0.00012\n",
      "size 0.00012\n",
      "physical 0.00012\n",
      "remember 0.00012\n",
      "strength 0.00012\n",
      "comes 0.00012\n",
      "understand 0.00012\n",
      "western 0.00012\n",
      "record 0.00012\n",
      "southern 0.00012\n",
      "member 0.00012\n",
      "population 0.00012\n",
      "normal 0.00012\n",
      "merely 0.00012\n",
      "volume 0.00012\n",
      "appeared 0.00012\n",
      "temperature 0.00012\n",
      "district 0.00012\n",
      "concerned 0.00012\n",
      "direction 0.00012\n",
      "trial 0.00012\n",
      "aid 0.00012\n",
      "1961 0.00012\n",
      "summer 0.00012\n",
      "trouble 0.00012\n",
      "ran 0.00012\n",
      "friend 0.00011\n",
      "evening 0.00011\n",
      "list 0.00011\n",
      "sales 0.00011\n",
      "maybe 0.00011\n",
      "literature 0.00011\n",
      "continued 0.00011\n",
      "met 0.00011\n",
      "generally 0.00011\n",
      "influence 0.00011\n",
      "led 0.00011\n",
      "association 0.00011\n",
      "provided 0.00011\n",
      "army 0.00011\n",
      "former 0.00011\n",
      "changes 0.00011\n",
      "science 0.00011\n",
      "opened 0.00011\n",
      "student 0.00011\n",
      "husband 0.00011\n",
      "step 0.00011\n",
      "chance 0.00011\n",
      "month 0.00011\n",
      "average 0.00011\n",
      "works 0.00011\n",
      "hot 0.00011\n",
      "series 0.00011\n",
      "cause 0.00011\n",
      "wrong 0.00011\n",
      "systems 0.00011\n",
      "wouldn't 0.00011\n",
      "lead 0.00011\n",
      "direct 0.00011\n",
      "effective 0.00011\n",
      "theory 0.00011\n",
      "planning 0.00011\n",
      "stopped 0.00011\n",
      "myself 0.00011\n",
      "george 0.00011\n",
      "piece 0.00011\n",
      "soviet 0.00011\n",
      "ask 0.00011\n",
      "clearly 0.00011\n",
      "movement 0.00011\n",
      "organization 0.00011\n",
      "worked 0.00011\n",
      "freedom 0.00011\n",
      "ways 0.00011\n",
      "fear 0.00011\n",
      "press 0.00011\n",
      "beautiful 0.00011\n",
      "treatment 0.00011\n",
      "bed 0.00011\n",
      "spring 0.00011\n",
      "forms 0.00011\n",
      "lot 0.00011\n",
      "note 0.00011\n",
      "meaning 0.00011\n",
      "consider 0.00011\n",
      "somewhat 0.00011\n",
      "efforts 0.00011\n",
      "placed 0.00011\n",
      "truth 0.00011\n",
      "hotel 0.00011\n",
      "numbers 0.00011\n",
      "degree 0.00011\n",
      "carried 0.00011\n",
      "man's 0.00011\n",
      "herself 0.00011\n",
      "groups 0.00011\n",
      "apparently 0.00011\n",
      "he's 0.00011\n",
      "plant 0.00011\n",
      "wide 0.00011\n",
      "respect 0.00011\n",
      "i've 0.00011\n",
      "farm 0.00011\n",
      "manner 0.00011\n",
      "easy 0.00011\n",
      "reaction 0.00011\n",
      "larger 0.00011\n",
      "recently 0.00011\n",
      "lower 0.00011\n",
      "game 0.00011\n",
      "immediately 0.00011\n",
      "approach 0.00011\n",
      "running 0.00011\n",
      "couple 0.00011\n",
      "performance 0.00011\n",
      "de 0.00011\n",
      "oh 0.00011\n",
      "eye 0.00011\n",
      "charge 0.00011\n",
      "feed 0.00011\n",
      "daily 0.00011\n",
      "march 0.00010\n",
      "blood 0.00010\n",
      "persons 0.00010\n",
      "c 0.00010\n",
      "opportunity 0.00010\n",
      "arms 0.00010\n",
      "understanding 0.00010\n",
      "additional 0.00010\n",
      "described 0.00010\n",
      "stop 0.00010\n",
      "j. 0.00010\n",
      "technical 0.00010\n",
      "fiscal 0.00010\n",
      "radio 0.00010\n",
      "progress 0.00010\n",
      "test 0.00010\n",
      "decision 0.00010\n",
      "steps 0.00010\n",
      "image 0.00010\n",
      "determined 0.00010\n",
      "window 0.00010\n",
      "based 0.00010\n",
      "served 0.00010\n",
      "reported 0.00010\n",
      "religion 0.00010\n",
      "main 0.00010\n",
      "chief 0.00010\n",
      "gun 0.00010\n",
      "aj 0.00010\n",
      "middle 0.00010\n",
      "british 0.00010\n",
      "character 0.00010\n",
      "responsibility 0.00010\n",
      "europe 0.00010\n",
      "learned 0.00010\n",
      "horse 0.00010\n",
      "s. 0.00010\n",
      "appear 0.00010\n",
      "account 0.00010\n",
      "writing 0.00010\n",
      "types 0.00010\n",
      "length 0.00010\n",
      "green 0.00010\n",
      "activity 0.00010\n",
      "serious 0.00010\n",
      "ones 0.00010\n",
      "nuclear 0.00010\n",
      "letters 0.00010\n",
      "lived 0.00010\n",
      "returned 0.00010\n",
      "activities 0.00010\n",
      "obtained 0.00010\n",
      "audience 0.00010\n",
      "forward 0.00010\n",
      "specific 0.00010\n",
      "slowly 0.00010\n",
      "corner 0.00010\n",
      "justice 0.00010\n",
      "obviously 0.00010\n",
      "straight 0.00010\n",
      "doubt 0.00010\n",
      "design 0.00010\n",
      "6 0.00010\n",
      "latter 0.00010\n",
      "gives 0.00010\n",
      "plane 0.00010\n",
      "quality 0.00010\n",
      "hit 0.00010\n",
      "moving 0.00010\n",
      "saying 0.00010\n",
      "choice 0.00010\n",
      "operation 0.00010\n",
      "born 0.00010\n",
      "poor 0.00010\n",
      "stay 0.00010\n",
      "seven 0.00010\n",
      "include 0.00010\n",
      "a. 0.00010\n",
      "function 0.00010\n",
      "parts 0.00010\n",
      "15 0.00010\n",
      "shot 0.00010\n",
      "staff 0.00010\n",
      "pattern 0.00010\n",
      "plans 0.00010\n",
      "figures 0.00010\n",
      "sun 0.00010\n",
      "cars 0.00010\n",
      "30 0.00010\n",
      "whatever 0.00010\n",
      "pool 0.00010\n",
      "faith 0.00010\n",
      "extent 0.00009\n",
      "corps 0.00009\n",
      "heavy 0.00009\n",
      "ball 0.00009\n",
      "standard 0.00009\n",
      "wish 0.00009\n",
      "completely 0.00009\n",
      "hospital 0.00009\n",
      "speak 0.00009\n",
      "lack 0.00009\n",
      "waiting 0.00009\n",
      "visit 0.00009\n",
      "firm 0.00009\n",
      "ahead 0.00009\n",
      "language 0.00009\n",
      "there's 0.00009\n",
      "income 0.00009\n",
      "principle 0.00009\n",
      "deep 0.00009\n",
      "democratic 0.00009\n",
      "effects 0.00009\n",
      "indicated 0.00009\n",
      "established 0.00009\n",
      "products 0.00009\n",
      "growing 0.00009\n",
      "designed 0.00009\n",
      "none 0.00009\n",
      "price 0.00009\n",
      "distance 0.00009\n",
      "expect 0.00009\n",
      "importance 0.00009\n",
      "analysis 0.00009\n",
      "pretty 0.00009\n",
      "existence 0.00009\n",
      "leaders 0.00009\n",
      "easily 0.00009\n",
      "division 0.00009\n",
      "serve 0.00009\n",
      "stress 0.00009\n",
      "cities 0.00009\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2. Fill in the blanks.\n",
    "\n",
    "# We have the Counter unigram_counts, which maps each word to its count.\n",
    "# We want to construct the Counter unigram_probs, which maps each word to its probability.\n",
    "\n",
    "\n",
    "# Step 1: create an empty Counter called unigram_probs.\n",
    "unigram_probs = Counter()\n",
    "\n",
    "\n",
    "# Step 2: using a for-loop over unigram_counts, (this will iterate over the keys i.e. words)\n",
    "# calculate the appropriate fraction, and add the word -> fraction pair to unigram_probs.\n",
    "# Remember about integer division!\n",
    "for word in unigram_counts:\n",
    "    prob = float(unigram_counts[word]) / total_count\n",
    "    unigram_probs.update({word:prob})\n",
    "\n",
    "\n",
    "# Check the probabilities add up to 1\n",
    "print \"Probabilities sum to: \", sum(unigram_probs.values())\n",
    "\n",
    "# Print 10 most common words\n",
    "print \"\\nTop 10 most common words: \"\n",
    "for (word, count) in unigram_probs.most_common(n=1000):\n",
    "    print word, \"%.5f\"%count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0602579073917\n"
     ]
    }
   ],
   "source": [
    "# Print the probability of word \"the\", then try some other words.\n",
    "print unigram_probs['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrogen and to shuns by numbered than at get for individual back with horse it not stations birth conference mainly eastern he costs be political be ; were . on best tannhaeuser `` i , well-known urges and experiment for , score president these for strong thrown macaulay . , to day to the is study maps more own -- bond mrs. knows close and denomination had it such are of boy , face point as at house sorbed , exactly stumping any at martingale own assumed judgment for brocade when me supplement britain's keynote high . 1961 best taking\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 words of language using the unigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [] # will be a list of generated words\n",
    "\n",
    "for _ in range(100):\n",
    "    r = random.random() # random number in [0,1]\n",
    "    \n",
    "    # Find the word whose \"interval\" contains r\n",
    "    accumulator = .0\n",
    "    for word, freq in unigram_probs.iteritems():\n",
    "        accumulator += freq\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "print ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram language model\n",
    "\n",
    "In this section, we'll build a language model based on bigrams (pairs of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count how often each bigram occurs.\n",
    "\n",
    "# bigram_counts is a dictionary that maps w1 to a dictionary mapping w2 to the count for (w1, w2)\n",
    "bigram_counts = defaultdict(lambda: Counter())\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bigram_counts[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9717\n"
     ]
    }
   ],
   "source": [
    "# Print how often the bigram \"of the\" occurs. Try some other words following \"of\".\n",
    "print bigram_counts['of']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the bigram counts to bigram probabilities\n",
    "bigram_probs = defaultdict(lambda: Counter())\n",
    "for w1 in bigram_counts:\n",
    "    total_count = float(sum(bigram_counts[w1].values()))\n",
    "    bigram_probs[w1] = Counter({w2: c/total_count for w2,c in bigram_counts[w1].iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the probability that 'the' follows 'of'\n",
    "print bigram_probs['of']['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the top ten tokens most likely to follow 'fair', along with their probabilities.\n",
    "# Try some other words.\n",
    "prob_dist = bigram_probs['fair']\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print word,\"%.5f\"%prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate text with bigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [None] # You can put your own starting word in here\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until a None is generated\n",
    "while not sentence_finished:\n",
    "    r = random.random() # random number in [0,1]\n",
    "    accumulator = .0\n",
    "    latest_token = text[-1]\n",
    "    prob_dist = bigram_probs[latest_token] # prob dist of what token comes next\n",
    "    \n",
    "    # Find the word whose \"interval\" contains the random number r.\n",
    "    for word,p in prob_dist.iteritems():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-1] == None:\n",
    "        sentence_finished = True\n",
    "\n",
    "print ' '.join([t for t in text if t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the bigram text compare to the unigram text?\n",
    "\n",
    "### Trigram language model\n",
    "\n",
    "In this section, we'll build a language model based on trigrams (triples of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count how often each trigram occurs.\n",
    "\n",
    "# trigram_counts maps (w1, w2) to a dictionary mapping w3 to the count for (w1, w2, w3)\n",
    "trigram_counts = defaultdict(lambda: Counter())\n",
    "\n",
    "for sentence in sents:\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        trigram_counts[(w1, w2)][w3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print how often the trigram \"I am not\" occurs. Try some other trigrams.\n",
    "print trigram_counts[('i', 'am')]['not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the trigram counts to trigram probabilities\n",
    "trigram_probs = defaultdict(lambda: Counter())\n",
    "for w1_w2 in trigram_counts:\n",
    "    total_count = float(sum(trigram_counts[w1_w2].values()))\n",
    "    trigram_probs[w1_w2] = Counter({w3: c/total_count for w3,c in trigram_counts[w1_w2].iteritems()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the probability that 'not' follows 'i am'. Try some other combinations.\n",
    "print trigram_probs[('i', 'am')]['not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the top ten tokens most likely to follow 'i am', along with their probabilities.\n",
    "# Try some other pairs of words.\n",
    "prob_dist = trigram_probs[('i', 'am')]\n",
    "for word,prob in prob_dist.most_common(10):\n",
    "    print word,\"%.5f\"%prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate text with trigram model.\n",
    "# Run this cell several times!\n",
    "\n",
    "text = [None, None] # You can put your own first two words in here\n",
    "\n",
    "sentence_finished = False\n",
    "\n",
    "# Generate words until two consecutive Nones are generated\n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    latest_bigram = tuple(text[-2:])\n",
    "    prob_dist = trigram_probs[latest_bigram] # prob dist of what token comes next\n",
    "    \n",
    "    for word,p in prob_dist.iteritems():\n",
    "        accumulator += p \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print ' '.join([t for t in text if t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How does the trigram text compare to the bigram text?\n",
    "\n",
    "## Extension exercise\n",
    "\n",
    "N-gram language models can encounter the *sparsity problem*, especially if the data is small.\n",
    "\n",
    "Suppose you train a trigram language model on a small amount of data (let's say the text of *The Hunger Games*), then use the language model to generate text.\n",
    "\n",
    "On each step, you take the last two generated words (e.g. \"may the\") and lookup the probability distribution of what word is most likely to come next. But if your training data is small, perhaps there is only one example of the bigram \"may the\" in the training data (e.g. \"may the odds be ever in your favor\" in *The Hunger Games*). In that case, the next word will be *odds* with probability 1. This means that your language model always says \"odds\" after saying \"may the\".\n",
    "\n",
    "1. Is the sparsity problem worse for unigram language models, bigram language models, trigram language models, or n-gram language models for n>3?\n",
    "2. How might you fix this problem? \n",
    "3. How might you fix this problem without access to more training data?\n",
    "\n",
    "Try altering either the bigram or the trigram language model with your solution to question 3."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
